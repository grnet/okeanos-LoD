#!/bin/bash

# Source environment file to make sure JAVA_HOME variable is available.
source /etc/environment

# Define the user that should run this script.
SCRIPT_USER=hduser

# The path where Apache Hadoop is installed.
INSTALLATION_PATH="{{ installation_path }}"

# The full path of the lock file to use.
LOCKFILE="$INSTALLATION_PATH/hadoop/hdfs-lock"

# The command that will start Apache HDFS.
START_COMMAND="$INSTALLATION_PATH/hadoop/sbin/start-dfs.sh"

# The command that will stop Apache HDFS.
STOP_COMMAND="$INSTALLATION_PATH/hadoop/sbin/stop-dfs.sh"

start(){
  # Assert that there is no other Apache HDFS instance, created with this script, running.
  [ -f $LOCKFILE ] && return

  # Execute the command to start Apache HDFS. The command waits until HDFS has been started.
  sudo -u $SCRIPT_USER $START_COMMAND > /dev/null

  # Get the returned value of the executed command and create a lock file to prevent multiple instantiations.
  RETVAL=$?
  [ $RETVAL -eq 0 ] && $(sudo -u $SCRIPT_USER touch $LOCKFILE)
  return $RETVAL
}

stop(){
  # Assert that an Apache HDFS instance, created with this script, is running.
  [ ! -f $LOCKFILE ] && return 0

  # Execute the command to stop Apache HDFS. The command waits until HDFS has been stopped.
  sudo -u $SCRIPT_USER $STOP_COMMAND > /dev/null

  # Get the returned value of the executed command and delete the lock file.
  RETVAL=$?
  [ $RETVAL -eq 0 ] && $(sudo -u $SCRIPT_USER rm -f $LOCKFILE)
  return $RETVAL
}

restart(){
  stop
  start
}

RETVAL=0

case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  restart|reload|force-reload)
    restart
    ;;
  condrestart)
    [ -f $LOCKFILE ] && restart || :
    ;;
  status)
    # If the lock file exists, then Apache HDFS is running.
    [ -f $LOCKFILE ] && echo "Apache HDFS is running." || echo "Apache HDFS is not running."
    RETVAL=$?
    ;;
  *)
    echo "Usage: $0 {start|stop|status|restart|reload|force-reload|condrestart}"
    RETVAL=1
esac

exit $RETVAL

